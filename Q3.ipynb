{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f42a3ce",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
    "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "\n",
    "\n",
    "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
    "\n",
    "\n",
    "**Student Name**: Parsa Yousefpoor\n",
    "\n",
    "**Student ID**: 400104686\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c559e",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "First we import libraries that we need for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4440caa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from keras) (0.4.0)\n",
      "Requirement already satisfied: rich in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from keras) (3.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from keras) (1.21.5)\n",
      "Requirement already satisfied: optree in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: namex in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: absl-py in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from optree->keras) (4.3.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\microsoft\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0423187e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T20:01:43.514540300Z",
     "start_time": "2024-04-28T20:01:43.507237400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from keras.models import Sequential\\nfrom keras.layers import Dense, Dropout\\nfrom keras.optimizers import RMSprop\\nfrom keras.constraints import maxnorm\\nfrom keras import regularizers'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "'''from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.constraints import maxnorm\n",
    "from keras import regularizers'''\n",
    "# import any other libraries needed below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f795731",
   "metadata": {},
   "source": [
    "## Reading Data and Preprocessing\n",
    "\n",
    "In this section, we want to read data from a CSV file and then preprocess it to make it ready for the rest of the problem.\n",
    "\n",
    "First, we read the data in the cell below and extract an $m \\times n$ matrix, $X$, and an $m \\times 1$ vector, $Y$, from it, which represent our knowledge about the features of the data (`X1`, `X2`, `X3`) and the class (`Y`), respectively. Note that by $m$, we mean the number of data points and by $n$, we mean the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "410e750d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-28T20:17:00.605170600Z",
     "start_time": "2024-04-28T20:17:00.560614300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "X, Y = None, None\n",
    "\n",
    "### START CODE HERE ###\n",
    "data = pd.read_csv('data_logistic.csv')\n",
    "X = data[['X1', 'X2', 'X3']]\n",
    "Y = data['Y']\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866734e2",
   "metadata": {},
   "source": [
    "Next, we should normalize our data. For normalizing a vector $\\mathbf{x}$, a very common method is to use this formula:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{norm} = \\dfrac{\\mathbf{x} - \\overline{\\mathbf{x}}}{\\sigma_\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "Here, $\\overline{x}$ and $\\sigma_\\mathbf{x}$ denote the mean and standard deviation of vector $\\mathbf{x}$, respectively. Use this formula and store the new $X$ and $Y$ vectors in the cell below.\n",
    "\n",
    "**Question**: Briefly explain why we need to normalize our data before starting the training.\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**First**: to standardize\n",
    "\n",
    "**Second**: to eliminate its effect in distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e757eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Microsoft\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3438: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "Y = (Y - np.mean(Y)) / np.std(Y)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465bfa4",
   "metadata": {},
   "source": [
    "Finally, we should add a column of $1$s at the beginning of $X$ to represent the bias term. Do this in the next cell. Note that after this process, $X$ should be an $m \\times (n+1)$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9a60f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "m, n = X.shape\n",
    "X = np.hstack((np.ones((m, 1)), X))\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf0d78",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8714abe",
   "metadata": {},
   "source": [
    "### Sigmoid Function\n",
    "You should begin by implementing the $\\sigma(\\mathbf{x})$ function. Recall that the logistic regression hypothesis $\\mathcal{h}()$ is defined as:\n",
    "$$\n",
    "\\mathcal{h}_{\\theta}(\\mathbf{x}) = \\mathcal{g}(\\theta^\\mathbf{T}\\mathbf{x})\n",
    "$$\n",
    "where $\\mathcal{g}()$ is the sigmoid function as:\n",
    "$$\n",
    "\\mathcal{g}(\\mathbf{z}) = \\frac{1}{1+exp^{-\\mathbf{z}}}\n",
    "$$\n",
    "The Sigmoid function has the property that $\\mathbf{g}(+\\infty)\\approx 1$ and $\\mathcal{g}(−\\infty)\\approx0$. Test your function by calling `sigmoid(z)` on different test samples. Be certain that your sigmoid function works with both vectors and matrices - for either a vector or a matrix, your function should perform the sigmoid function on every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a6b6ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(self, Z):\n",
    "    '''\n",
    "    Applies the sigmoid function on every element of Z\n",
    "    Arguments:\n",
    "        Z can be a (n,) vector or (n , m) matrix\n",
    "    Returns:\n",
    "        A vector/matrix, same shape with Z, that has the sigmoid function applied elementwise\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83846074",
   "metadata": {},
   "source": [
    "### Cost Function \n",
    "Implement the functions to compute the cost function. Recall the cost function for logistic regression is a scalar value given by:\n",
    "$$\n",
    "\\mathcal{J}(\\theta) = \\sum_{i=1}^{n}[-y^{(i)}\\log{(\\mathcal{h}_\\theta(\\mathbf{x}^{(i)}))}-(1-y^{(i)})\\log{(1-\\mathcal{h}_\\theta(\\mathbf{x}^{(i)}))}] + \\frac{\\lambda}{2}||\\theta||_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a9bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(theta, X, y, regLambda):\n",
    "    '''\n",
    "    Computes the objective function\n",
    "    Arguments:\n",
    "        theta is d-dimensional numpy vector\n",
    "        X is a n-by-d numpy matrix\n",
    "        y is an n-dimensional numpy vector\n",
    "        regLambda is the scalar regularization constant\n",
    "    Returns:\n",
    "        a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "    '''\n",
    "    \n",
    "    m, n = X.shape\n",
    "    loss = None\n",
    "    ### START CODE HERE ###\n",
    "    m = len(y)\n",
    "    h = 1 / (1 + np.exp(-(X @ theta)))  # Predicted probabilities\n",
    "    term1 = -y * np.log(h)\n",
    "    term2 = -(1 - y) * np.log(1 - h)\n",
    "    regularization_term = (regLambda / (2 * m)) * np.sum(theta[1:]**2)  # Exclude bias term\n",
    "\n",
    "    # Compute the cost (without regularization)\n",
    "    cost = (1 / m) * np.sum(term1 + term2)\n",
    "\n",
    "    # Add regularization term\n",
    "    cost += regularization_term\n",
    "\n",
    "    loss = cost\n",
    "    ### END CODE HERE ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf1146",
   "metadata": {},
   "source": [
    "### Gradient of the Cost Function\n",
    "Now, we want to calculate the gradient of the cost function. The gradient of the cost function is a d-dimensional vector.\\\n",
    "We must be careful not to regularize the $\\theta_0$ parameter (corresponding to the first feature we add to each instance), and so the 0's element is given by:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{J}(\\theta)}{\\partial \\theta_0} = \\sum_{i=1}^n (\\mathcal{h}_\\theta(\\mathbf{x}^{(i)})-y^{(i)})\n",
    "$$\n",
    "\n",
    "Question: What is the answer to this problem for the $j^{th}$ element (for $j=1...d$)?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35f7c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeGradient(theta, X, y, regLambda):\n",
    "    '''\n",
    "    Computes the gradient of the objective function\n",
    "    Arguments:\n",
    "        theta is d-dimensional numpy vector\n",
    "        X is a n-by-d numpy matrix\n",
    "        y is an n-dimensional numpy vector\n",
    "        regLambda is the scalar regularization constant\n",
    "    Returns:\n",
    "        the gradient, an d-dimensional vector\n",
    "    '''\n",
    "    \n",
    "    m, n = X.shape\n",
    "    grad = None\n",
    "    ### START CODE HERE ###\n",
    "    m, n = X.shape  # Number of training examples and features\n",
    "    grad = np.zeros(n)  # Initialize gradient vector\n",
    "    \n",
    "    # Compute predictions (hypothesis)\n",
    "    y_pred = np.dot(X, theta)\n",
    "    \n",
    "    # Compute the gradient\n",
    "    grad = -(1/m) * np.dot(X.T, y - y_pred) + (regLambda/m) * theta\n",
    "    ### END CODE HERE ###\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc86bd",
   "metadata": {},
   "source": [
    "### Training and Prediction\n",
    "Once you have the cost and gradient functions complete, implement the fit and predict methods.\\\n",
    "Your fit method should train the model via gradient descent, relying on the cost and gradient functions. This function should return two parameters. The first parameter is $\\theta$, and the second parameter is a `numpy` array that contains the loss in each iteration. This array is indicated by `loss_history` in the code.\\\n",
    "Instead of simply running gradient descent for a specific number of iterations, we will use a more sophisticated method: we will stop it after the solution hasconverged. Stop the gradient descent procedure when $\\theta$ stops changing between consecutive iterations. You can detect this convergence when:\n",
    "$$\n",
    "||\\theta_{new}-\\theta_{old}||_2 <= \\epsilon,\n",
    "$$\n",
    "for some small $\\epsilon$ (e.g, $\\epsilon=10E-4$).\\\n",
    "For readability, we’d recommend implementing this convergence test as a dedicated function `hasConverged`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc0cad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, regLambda = 0.01, alpha = 0.01, epsilon = 1e-4, maxNumIters = 100):\n",
    "    '''\n",
    "    Trains the model\n",
    "    Arguments:\n",
    "        X           is a n-by-d numpy matrix\n",
    "        y           is an n-dimensional numpy vector\n",
    "        maxNumIters is the maximum number of gradient descent iterations\n",
    "        regLambda   is the scalar regularization constant\n",
    "        epsilon     is the convergence rate\n",
    "        alpha       is the gradient descent learning rate\n",
    "    '''\n",
    "    \n",
    "    m, n = X.shape\n",
    "    theta, loss_history = None, None \n",
    "    ### START CODE HERE ###\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)  # Initialize theta with zeros\n",
    "    loss_history = []    # To store loss values in each iteration\n",
    "    \n",
    "    for _ in range(maxNumIters):\n",
    "        # Compute the gradient\n",
    "        gradient = computeGradient(theta, X, y, regLambda)\n",
    "        \n",
    "        # Update theta using gradient descent\n",
    "        theta -= alpha * gradient\n",
    "        \n",
    "        # Compute the cost (loss) and append it to loss_history\n",
    "        cost = computeCost(theta, X, y, regLambda)\n",
    "        loss_history.append(cost)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if hasConverged(theta, theta - alpha * gradient, epsilon):\n",
    "            break\n",
    "    loss_history = np.array(loss_history)\n",
    "    ### END CODE HERE ###\n",
    "    return theta, loss_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hasConverged(theta_old, theta_new, epsilon):\n",
    "    '''\n",
    "    Return if the theta converged or not\n",
    "    Arguments:\n",
    "        theta_old   is the theta calculated in prevoius iteration\n",
    "        theta_new   is the theta calculated in current iteration\n",
    "        epsilon     is the convergence rate\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    if np.linalg.norm(theta_new - theta_old) <= epsilon:\n",
    "        return True\n",
    "    ### END CODE HERE ###\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb648852",
   "metadata": {},
   "source": [
    "Finally, we want to evaluate our loss for this problem. Complete the cell below to calculate and print the loss of each iteration and the final theta of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "252e556f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = 0.686920\n",
      "Iteration 2: Loss = 0.680771\n",
      "Iteration 3: Loss = 0.674700\n",
      "Iteration 4: Loss = 0.668705\n",
      "Iteration 5: Loss = 0.662785\n",
      "Iteration 6: Loss = 0.656939\n",
      "Iteration 7: Loss = 0.651166\n",
      "Iteration 8: Loss = 0.645466\n",
      "Iteration 9: Loss = 0.639836\n",
      "Iteration 10: Loss = 0.634276\n",
      "Iteration 11: Loss = 0.628786\n",
      "Iteration 12: Loss = 0.623363\n",
      "Iteration 13: Loss = 0.618008\n",
      "Iteration 14: Loss = 0.612719\n",
      "Iteration 15: Loss = 0.607495\n",
      "Iteration 16: Loss = 0.602335\n",
      "Iteration 17: Loss = 0.597239\n",
      "Iteration 18: Loss = 0.592206\n",
      "Iteration 19: Loss = 0.587234\n",
      "Iteration 20: Loss = 0.582323\n",
      "Iteration 21: Loss = 0.577473\n",
      "Iteration 22: Loss = 0.572681\n",
      "Iteration 23: Loss = 0.567948\n",
      "Iteration 24: Loss = 0.563272\n",
      "Iteration 25: Loss = 0.558653\n",
      "Iteration 26: Loss = 0.554091\n",
      "Iteration 27: Loss = 0.549583\n",
      "Iteration 28: Loss = 0.545130\n",
      "Iteration 29: Loss = 0.540731\n",
      "Iteration 30: Loss = 0.536385\n",
      "Iteration 31: Loss = 0.532091\n",
      "Iteration 32: Loss = 0.527848\n",
      "Iteration 33: Loss = 0.523657\n",
      "Iteration 34: Loss = 0.519516\n",
      "Iteration 35: Loss = 0.515424\n",
      "Iteration 36: Loss = 0.511381\n",
      "Iteration 37: Loss = 0.507386\n",
      "Iteration 38: Loss = 0.503439\n",
      "Iteration 39: Loss = 0.499539\n",
      "Iteration 40: Loss = 0.495685\n",
      "Iteration 41: Loss = 0.491877\n",
      "Iteration 42: Loss = 0.488114\n",
      "Iteration 43: Loss = 0.484395\n",
      "Iteration 44: Loss = 0.480720\n",
      "Iteration 45: Loss = 0.477089\n",
      "Iteration 46: Loss = 0.473500\n",
      "Iteration 47: Loss = 0.469954\n",
      "Iteration 48: Loss = 0.466449\n",
      "Iteration 49: Loss = 0.462985\n",
      "Iteration 50: Loss = 0.459562\n",
      "Iteration 51: Loss = 0.456178\n",
      "Iteration 52: Loss = 0.452834\n",
      "Iteration 53: Loss = 0.449529\n",
      "Iteration 54: Loss = 0.446263\n",
      "Iteration 55: Loss = 0.443034\n",
      "Iteration 56: Loss = 0.439843\n",
      "Iteration 57: Loss = 0.436689\n",
      "Iteration 58: Loss = 0.433572\n",
      "Iteration 59: Loss = 0.430490\n",
      "Iteration 60: Loss = 0.427444\n",
      "Iteration 61: Loss = 0.424433\n",
      "Iteration 62: Loss = 0.421457\n",
      "Iteration 63: Loss = 0.418515\n",
      "Iteration 64: Loss = 0.415607\n",
      "Iteration 65: Loss = 0.412732\n",
      "Iteration 66: Loss = 0.409890\n",
      "Iteration 67: Loss = 0.407081\n",
      "Iteration 68: Loss = 0.404303\n",
      "Iteration 69: Loss = 0.401558\n",
      "Iteration 70: Loss = 0.398843\n",
      "Iteration 71: Loss = 0.396160\n",
      "Iteration 72: Loss = 0.393507\n",
      "Iteration 73: Loss = 0.390884\n",
      "Iteration 74: Loss = 0.388291\n",
      "Iteration 75: Loss = 0.385728\n",
      "Iteration 76: Loss = 0.383193\n",
      "Iteration 77: Loss = 0.380687\n",
      "Iteration 78: Loss = 0.378209\n",
      "Iteration 79: Loss = 0.375759\n",
      "Iteration 80: Loss = 0.373337\n",
      "Iteration 81: Loss = 0.370942\n",
      "Iteration 82: Loss = 0.368574\n",
      "Iteration 83: Loss = 0.366233\n",
      "Iteration 84: Loss = 0.363918\n",
      "Iteration 85: Loss = 0.361629\n",
      "Iteration 86: Loss = 0.359365\n",
      "Iteration 87: Loss = 0.357127\n",
      "Iteration 88: Loss = 0.354914\n",
      "Iteration 89: Loss = 0.352725\n",
      "Iteration 90: Loss = 0.350561\n",
      "Iteration 91: Loss = 0.348421\n",
      "Iteration 92: Loss = 0.346305\n",
      "Iteration 93: Loss = 0.344212\n",
      "Iteration 94: Loss = 0.342143\n",
      "Iteration 95: Loss = 0.340096\n",
      "Iteration 96: Loss = 0.338073\n",
      "Iteration 97: Loss = 0.336071\n",
      "Iteration 98: Loss = 0.334092\n",
      "Iteration 99: Loss = 0.332134\n",
      "Iteration 100: Loss = 0.330199\n",
      "Final theta: [-2.24991671e-15 -3.28567956e-01  3.57952818e-01  1.11057015e-01]\n"
     ]
    }
   ],
   "source": [
    "theta, loss_history = fit(X, Y) # calculating theta and loss of each iteration\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Print the loss for each iteration\n",
    "for i, loss in enumerate(loss_history):\n",
    "    print(f\"Iteration {i+1}: Loss = {loss:.6f}\")\n",
    "\n",
    "# Print the final theta\n",
    "print(f\"Final theta: {theta}\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3fab6",
   "metadata": {},
   "source": [
    "### Testing Your Implementation\n",
    "To test your logistic regression implementation, first you should use `train_test_split` function to split dataset into three parts:\n",
    "\n",
    "- 70% for the training set\n",
    "- 20% for the validation set\n",
    "- 10% for the test set\n",
    "\n",
    "Do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4518fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = None, None, None, None, None, None\n",
    "\n",
    "### START CODE HERE ###\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.33, random_state=42)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbe5d7",
   "metadata": {},
   "source": [
    "Then, you should complete `predict` function to find the weight vector and the loss on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95c2fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, theta):\n",
    "    '''\n",
    "    Use the model to predict values for each instance in X\n",
    "    Arguments:\n",
    "        theta is d-dimensional numpy vector\n",
    "        X     is a n-by-d numpy matrix\n",
    "    Returns:\n",
    "        an n-dimensional numpy vector of the predictions, the output should be binary (use h_theta > .5)\n",
    "    '''\n",
    "    \n",
    "    Y = None\n",
    "    ### START CODE HERE ###\n",
    "    # Compute the dot product of X and theta\n",
    "    z = np.dot(X, theta)\n",
    "    \n",
    "    # Apply the sigmoid function to get probabilities\n",
    "    h_theta = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    Y = (h_theta > 0.5).astype(int)\n",
    "    ### END CODE HERE ###\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d1c02",
   "metadata": {},
   "source": [
    "Now, run the `fit` and `predict` function for different values of the learning rate and regularization constant. Plot the `loss_history` of these different values for train and test data both in the same figure.\n",
    "\n",
    "**Question**: Discuss the effect of the learning rate and regularization constant and find the best values of these parameters.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd2af382",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13436\\2091008610.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mY_train_binary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_binary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Create a new model for each fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     history = model.fit(X_train[train], Y_train_binary[train], validation_data=(X_train[val], Y_train_binary[val]),\n\u001b[0;32m     26\u001b[0m                         epochs=40, batch_size=50, verbose=0)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13436\\2091008610.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(neurons, init_mode, activation, inputDim, dropout_rate, learn_rate, momentum, weight_constraint)\u001b[0m\n\u001b[0;32m      2\u001b[0m def create_model(neurons=379, init_mode='uniform', activation='relu', inputDim=8040,\n\u001b[0;32m      3\u001b[0m                  dropout_rate=1.1, learn_rate=0.001, momentum=0.7, weight_constraint=6):\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     model.add(Dense(neurons, input_dim=inputDim, kernel_initializer=init_mode,\n\u001b[0;32m      6\u001b[0m                     \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_constraint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "def create_model(neurons=379, init_mode='uniform', activation='relu', inputDim=8040,\n",
    "                 dropout_rate=1.1, learn_rate=0.001, momentum=0.7, weight_constraint=6):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=inputDim, kernel_initializer=init_mode,\n",
    "                    activation=activation, kernel_constraint=maxnorm(weight_constraint),\n",
    "                    kernel_regularizer=regularizers.l2(0.002)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = RMSprop(lr=learn_rate)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define k-fold cross-validation test harness\n",
    "seed = 7\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "# Initialize lists to store loss history\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "Y_train_binary = (Y_train > Y_train.median()).astype(int)\n",
    "for train, val in kfold.split(X_train, Y_train_binary):\n",
    "    model = create_model()  # Create a new model for each fold\n",
    "    history = model.fit(X_train[train], Y_train_binary[train], validation_data=(X_train[val], Y_train_binary[val]),\n",
    "                        epochs=40, batch_size=50, verbose=0)\n",
    "    train_loss_history.append(history.history['loss'])\n",
    "    val_loss_history.append(history.history['val_loss'])\n",
    "\n",
    "# Plot Model Loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(train_loss_history)):\n",
    "    plt.plot(train_loss_history[i], label=f\"Fold {i+1} (Train)\")\n",
    "    plt.plot(val_loss_history[i], label=f\"Fold {i+1} (Validation)\")\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11babf15",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "In this part, you will use the `GaussianNB` classifier to classify the data. You should not change the default parameters of this classifier. First, train the classifier on the training set and then find the accuracy of it on the test set.\n",
    "\n",
    "**Question**: What is the accuracy of this method on test set?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ef450fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371657d",
   "metadata": {},
   "source": [
    "## LDA (Linear Discriminant Analysis)\n",
    "\n",
    "In this part, you will use the `LinearDiscriminantAnalysis` classifier to classify the data. You should not change the default parameters of this classifier. First, train the classifier on the training set and then find the accuracy of it on the test set.\n",
    "\n",
    "**Question**: What is the accuracy of this method on test set?\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cc8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47736bdf",
   "metadata": {},
   "source": [
    "## Conclution\n",
    "\n",
    "**Question**: What is the best method for classifying this dataset? What is the best accuracy on the test set?\n",
    "\n",
    "**Answer**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
